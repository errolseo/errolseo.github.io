<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script type="application/ld+json">

{  
  "@context":"http://schema.org",
  "@type":"Website",
  "@id":"https:\/\/errolseo.github.io\/",
  "author": {
    "@type": "Person",
    "name": "Errol Seo",
    
    "image": "https://errolseo.github.io/images/profile.webp"
    
  },
  "name":"Errol\u0027s AI Lab",
  "description":"Qwen3-Next 모델의 핵심인 Gated Delta Networks에 대한 설명",
  "url":"https:\/\/errolseo.github.io\/post\/llm\/2\/",
  "keywords":"[]"
}

</script>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.151.0 with theme Tranquilpeak 0.5.3-BETA">
<meta name="author" content="Errol Seo">
<meta name="keywords" content="">
<meta name="description" content="Qwen3-Next 모델의 핵심인 Gated Delta Networks에 대한 설명">


<meta property="og:description" content="Qwen3-Next 모델의 핵심인 Gated Delta Networks에 대한 설명">
<meta property="og:type" content="article">
<meta property="og:title" content="Gated Delta Networks">
<meta name="twitter:title" content="Gated Delta Networks">
<meta property="og:url" content="https://errolseo.github.io/post/llm/2/">
<meta property="twitter:url" content="https://errolseo.github.io/post/llm/2/">
<meta property="og:site_name" content="Errol&#39;s AI Lab">
<meta property="og:description" content="Qwen3-Next 모델의 핵심인 Gated Delta Networks에 대한 설명">
<meta name="twitter:description" content="Qwen3-Next 모델의 핵심인 Gated Delta Networks에 대한 설명">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2025-09-19T00:00:00">
  
  
    <meta property="article:modified_time" content="2025-09-19T00:00:00">
  
  
  
    
      <meta property="article:section" content="LLM">
    
  
  


<meta name="twitter:card" content="summary">







  <meta property="og:image" content="https://errolseo.github.io/images/profile.webp">
  <meta property="twitter:image" content="https://errolseo.github.io/images/profile.webp">




  <meta property="og:image" content="https://errolseo.github.io/images/LLM/2/1.webp">
  <meta property="twitter:image" content="https://errolseo.github.io/images/LLM/2/1.webp">


    <title>Gated Delta Networks</title>

    <link rel="icon" href="https://errolseo.github.io/images/favicon.png">
    

    

    <link rel="canonical" href="https://errolseo.github.io/post/llm/2/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" integrity="sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xLiPY/NS5R+E6ztJQ==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Color+Emoji&family=Noto+Sans+KR&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200&icon_names=search" />
    
    
    <link rel="stylesheet" href="https://errolseo.github.io/css/style-vd5bdnipxtam3iir3ip2zpgrpnpfztfmpfispblr4wjmh51ksew8kklhdp.min.css" />
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://errolseo.github.io/" aria-label="Go to homepage">Errol&#39;s AI Lab</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://errolseo.github.io/about" aria-label="Open the link: /about">
    
    
    
      
        <img class="header-picture" src="https://errolseo.github.io/images/profile.webp" alt="Author&#39;s picture" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <img class="sidebar-profile-picture" src="https://errolseo.github.io/images/profile.webp" alt="Author&#39;s picture" />
        <h4 class="sidebar-profile-name">Errol Seo</h4>
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://errolseo.github.io/" title="Home">
    
      <i class="sidebar-button-icon fas fa-lg fa-home" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://errolseo.github.io/categories" title="Categories">
    
      <i class="sidebar-button-icon fas fa-lg fa-list" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://errolseo.github.io/archives" title="Archives">
    
      <i class="sidebar-button-icon fas fa-lg fa-folder-open" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://errolseo.github.io/about" title="About">
    
      <i class="sidebar-button-icon fas fa-lg fa-user" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/errolseo" target="_blank" rel="noopener" title="GitHub">
    
      <i class="sidebar-button-icon fab fa-lg fa-github" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      <div id="main" data-behavior="5">
        <article class="post" id="top">
          <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title">
      Gated Delta Networks
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time datetime="2025-09-19T00:00:00Z">
        
  September 19, 2025

      </time>
    
    
  
  
    <span>in</span>
    
      <a href="https://errolseo.github.io/categories/llm">LLM</a>
    
  

  </div>

</div>
          <div class="post-content markdown">
            <div class="main-content-wrap">
              <p>2025년 9월, Qwen3-Next 모델이 공개되었습니다. 새롭게 공개된 Qwen3-Next-80B-A3B 모델은 기존 Qwen3-32B 모델보다 속도면에서는 10.7배 빠른 추론 속도를, 성능면에서는 각종 벤치마크에서 더 높은 점수를 기록했습니다.</p>
<p>가장 주목할 만한 점은 10배 이상 향상된 추론 속도입니다. 이는 <strong>Gated Delta Networks</strong>라는 새로운 선형 트랜스포머(Linear Transformer) 구조를 채택한 덕분입니다. 이 구조는 특히 긴 컨텍스트(long context)를 처리할 때 뛰어난 성능 개선을 보여주며 기존 언어 모델들의 한계를 극복했습니다.</p>



<div class="figure fig-80 center" >
  
    <img class="fig-img" src="https://errolseo.github.io/images/LLM/2/1.webp" >
  
  
</div>

<blockquote>
<p>그림 1. Hybrid Architecture (Qwen3-Next)<br>
<a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list">QwenTeam (2025)</a></p></blockquote>
<p>Qwen3-Next 모델은 그림 1과 같이 Gated DeltaNet과 Gated Attention을 결합한 하이브리드 구조를 채택했습니다. 동시에 Output gating 메커니즘(그림 1의 Output Gate)을 활용하여 어텐션(Attention)의 low-rank 이슈를 개선하는 등 다양한 기술적 시도를 담아냈습니다.</p>
<p>또한, 80B의 파라미터 중 3.7%에 해당하는 약 3B의 파라미터만 사용하는 Ultra-Sparse MoE를 성공적으로 학습하며 효율성을 극대화했습니다.</p>
<p>마지막으로, 다중 토큰 예측(Multi-Token Prediction, MTP)을 도입하여, 기존 모델이 매 스텝(step)마다 하나의 토큰을 순차적으로 생성했던 것과 달리, <span class="highlight-text primary">Qwen3-Next는 스텝마다 여러 개의 토큰을 예측할 수 있게 되었습니다.</span></p>
<hr>
<h2 id="mamba2">Mamba2</h2>
<p>Gated Delta Network는 Delta Network에 Mamba2의 Gating 메커니즘을 더한 구조입니다. 이름에서 알 수 있듯 Mamba2의 기반이 되는 Mamba1 모델도 있지만, Mamba2를 이해하는 데 필수적인 내용은 아니므로 설명은 생략하겠습니다.</p>
<p>Mamba2는 기존 트랜스포머의 주요 병목이었던 <strong>이차 복잡도(quadratic complexity, $O(L^2)$)</strong> 문제를 <strong>선형 복잡도(linear complexity, $O(L)$)</strong>로 개선하기 위해 제안된 모델입니다. 핵심 아이디어는 시퀀스(Sequence) 데이터를 처리할 때 어텐션(Attention)을 사용하는 대신, <strong>상태(State)</strong> 개념을 도입하여 순차적으로 정보를 누적하는 것입니다. 이는 Residual Connection과 유사한 방식입니다.</p>
<p>$$
\tag{1} S_t = \alpha_t S_{t-1} + v_t k_t^\intercal \in \R^{d_v \times d_k}
$$</p>
<p>$$
\tag{2} o_t = S_t q_t = \sum^t_{i=1} (v_i k_i^\intercal) q_t = \sum^t_{i=1} v_i (k_i^\intercal q_t) \in \R^{d_v}
$$</p>
<p>$$
\tag{3} O = (QK^\intercal \odot M) V \in \R^{L \times d_v}
$$
위 수식을 통해 Mamba2의 작동 방식을 살펴보겠습니다.</p>
<p><strong>수식 (1)</strong>은 상태($S$)가 업데이트되는 과정을 보여줍니다. 현재 타임스텝($t$)의 상태($S_t$)는 이전 상태($S_{t-1}$)에 <strong>감쇠율(decay rate, $\alpha_t$)</strong>을 곱해 과거 정보의 영향력을 조절하고, 현재 토큰의 Key-Value 벡터($v_t k_t^\intercal$)를 더하여 계산됩니다. 이처럼 이전 정보를 선별적으로 유지하며 다음 상태로 넘겨주는 순환적인 구조를 가집니다.</p>
<p><strong>수식 (2)</strong>에서는 이렇게 업데이트된 상태($S_t$)와 현재 토큰의 쿼리($q_t$)를 곱해 최종 출력($o_t$)을 계산합니다. 최종적으로는 Query, Key, Value가 모두 계산에 사용되므로, <strong>수식 (3)</strong>으로 표현되는 표준 트랜스포머의 어텐션 메커니즘과 유사한 역할을 수행합니다. <span class="highlight-text primary">그렇다면 연산량의 차이는 어디서 발생할까요?</span></p>
<p>핵심은 상태($S_t$)의 차원에 있습니다. Mamba2의 상태($S_t$)는 시퀀스 길이($L$)와 무관하게 $d_v \times d_k$라는 고정된 크기를 가집니다. 따라서 각 토큰을 처리하는 연산은 시퀀스가 길어져도 일정하게 유지되며, 전체 시퀀스를 처리하는 데 필요한 총연산량은 시퀀스 길이에 선형적으로 비례($O(L)$)합니다.</p>
<p>반면, 표준 트랜스포머는 <strong>수식 (3)</strong>에서 볼 수 있듯, 시퀀스 내의 모든 토큰 쌍에 대한 유사도($Q K^\intercal$)를 계산해야 합니다. 이로 인해 시퀀스 길이가 길어질수록 연산량이 제곱($L \times L$)으로 늘어나는 이차 복잡도($O(L^2)$) 문제가 발생합니다. 바로 이 지점이 Mamba2가 트랜스포머 대비 긴 시퀀스에서 압도적인 효율성을 보이는 이유입니다.</p>
<hr>
<h2 id="delta-network">Delta Network</h2>
<p>Mamba2의 상태(State) 업데이트는 이전 상태에 감쇠율($\alpha_t$)을 곱하는 비교적 단순한 방식이었습니다. 반면 <strong>Delta Network</strong>는 <strong>Delta Update Rule</strong>을 통해 이전 상태($S_{t−1}$)에서 <span class="highlight-text primary">불필요한 정보는 지우고 새로운 정보는 선별적으로 쓰는 동적인 메모리 업데이트 메커니즘을 사용합니다.</span></p>
<p>$$
\tag{4} S_t = S_{t-1} - \underbrace{(S_{t-1} k_t)k_t^\intercal}_ {v_t^{old}} + \underbrace{(\beta_t v_t + (1 - \beta_t) S_{t-1} k_t)) k_t^\intercal}_ {v_t^{new}} = S_{t-1} (I - \beta_t k_t k_t^\intercal) + \beta_t v_t k_t^\intercal
$$</p>
<p><strong>수식 (4)</strong>를 살펴보면, 새로운 상태($S_t$)는 이전 상태($S_{t-1}$)에서 <span class="highlight-text primary">잊어야 할 과거 정보($v_t^{old}$)를 빼고, 새롭게 쓸 정보($v_t^{new}$)를 더하는</span> 직관적인 형태로 계산됩니다. 이 수식을 정리한 가장 오른쪽 항을 보면 Delta Network의 핵심 원리를 파악할 수 있습니다. 바로 ($I - \beta_t k_t k_t^\intercal$) 항으로 <strong>하우스홀더 변환(Householder transformation)</strong> 행렬에서 영감을 받은 형태입니다.</p>
<p>본래의 하우스홀더 행렬은 벡터를 특정 축에 대해 반사(reflection)시키는 직교 행렬(orthogonal matrix)로, 벡터의 길이를 보존하는 성질이 있습니다. 델타넷은 하우스홀더 변환 행렬과 유사한 Key 벡터의 외적 항($k_t k_t^\intercal$)을 포함하고 있지만, 직교성을 보존하지 않는다는 결정적인 차이가 있습니다. 이는 곧 벡터의 길이를 보존하지 않는다는 의미이며, 덕분에 델타넷은 단순히 정보를 반사하는 것을 넘어, 정보의 중요도에 따라 선택적으로 기억을 강화하거나 약화시키는 유연한 변환을 수행할 수 있게 됩니다.</p>
<hr>
<h2 id="gated-delta-network">Gated Delta Network</h2>
<p>Gated Delta Network는 이름에서 알 수 있듯, Delta Network의 구조에 Mamba2의 Gating 메커니즘을 추가한 형태입니다.</p>
<p>$$
\tag{5} S_t = S_{t-1} (\alpha_t (I - \beta_t k_t k_t^\intercal)) + \beta_t v_t k_t^\intercal
$$</p>
<p>수식 (5)는 앞서 살펴본 Delta Network의 수식 (4)에서 이전 상태($S_{t-1}$)에 감쇠율($\alpha_t$)을 곱해주는 항이 추가된 구조입니다. 여기서 한 가지 의문이 생길 수 있습니다. Delta Network는 정보를 <strong>선택적으로</strong> 지우기 위해 고안되었는데, 왜 전체 정보를 일괄적으로 잊게 하는 게이팅 메커니즘을 추가한 것일까요? 이는 언뜻 보기에 역설적입니다. <span class="highlight-text primary">이러한 의문은 Delta Network의 핵심 역할을 어떻게 바라보느냐에 따라 해소됩니다.</span></p>
<p>Delta Network의 하우스홀더 변환 기반 업데이트는 단순히 정보를 지우거나 더하는 행위가 아닙니다. 만약 아무런 변형 없이 이전 상태($S_{t−1}$)의 정보를 계속해서 누적하기만 한다면, 학습 과정에서 기울기(gradient)가 소실되거나 폭발할 위험이 커집니다.</p>
<p>즉, <span class="highlight-text primary">Delta Network는 이러한 기울기 문제를 텐서의 변형을 통해 해결하면서 이전 상태의 정보를 다음 상태로 안정적으로 전달하는 것이 주된 목적입니다.</span> 그러다 보니 정보 &lsquo;보존&rsquo;에는 탁월한 능력을 보이지만, 역설적으로 정보를 &lsquo;잊는&rsquo;데는 비효율적일 수 있습니다. 기본적으로 보존에 초점이 맞춰져 있기 때문입니다.</p>
<p>따라서 여기에 Mamba2의 게이팅 메커니즘($\alpha_t$)을 추가하여, 필요할 때 과거 정보를 효율적으로 잊을 수 있는 능력을 보완해 주는 것입니다. 결과적으로 Gated Delta Network는 정보를 안정적으로 보존하는 동시에 효과적으로 잊는, 두 가지 능력을 모두 갖춘 균형 잡힌 구조가 됩니다.</p>
<hr>
<h2 id="chunkwise-parallel-form">Chunkwise parallel form</h2>
<p>Chunkwise Parallelism이란, 이름 그대로 기존에 단일 상태(state) 단위로 순차 처리하던 작업을 청크(chunk) 단위로 묶어 병렬로 처리하는 기법입니다. 기존의 선형 어텐션이나 RNN 계열 모델에서 상태 ($S_t$)는 다음과 같이 계산됩니다.</p>
<p>$$
S_t = S_{t-1} + \Delta S_t
$$</p>
<p>여기서 문제는 $S_t$ 를 계산하려면 반드시 $S_{t-1}$ 의 계산이 완료되어야 한다는 <strong>순차적 의존성</strong>입니다. 이러한 방식은 GPU의 가장 큰 장점인 병렬 연산 능력을 거의 활용하지 못합니다. 상태 변화량($\Delta S_1, \Delta S_2, \dots, \Delta S_n$) 자체는 모든 토큰에 대해 한 번의 병렬 연산으로 처리할 수 있지만, 이를 누적하는 과정($\Delta S_1 = S_0 + \Delta S_1, S_2 = S_1 + \Delta S_2, \dots, S_n = S_{n-1} + \Delta S_n$)은 GPU의 수많은 코어 중 단 하나만을 사용하여 순서대로 처리해야 합니다.</p>
<br>
<span class="highlight-text primary">이 문제를 벽돌 1,000개를 쌓는 작업에 비유해 보겠습니다.</span> 기존 방식은 한 명의 작업자가 벽돌 1,000개를 순서대로 쌓습니다. Chunkwise 방식은 10명의 작업자가 각자 100개씩 벽돌 더미를 동시에 쌓고, 마지막에 그 10개의 더미를 순서대로 합칩니다.
<p>당연히 10명이 협력하는 후자의 방식이 훨씬 빠릅니다. Chunkwise Parallelism은 이와 같이 병렬 처리 구간을 최대한 늘리고, 순차 처리 구간을 최소화하는 전략입니다.</p>
<p>이를 똑같이 선형 트랜스포머에 적용하면, <span class="highlight-text primary">기존 $O(N)$의 복잡도가 $O(N/C + logC)$로 개선됩니다. N이 1,000이고 C가 100이면 50배 빨라지는 것 입니다.</span></p>
<hr>
<h2 id="multi-token-prediction">Multi Token Prediction</h2>
<p><strong>다중 토큰 예측(Multi-Token Prediction, MTP)</strong>이라는 개념을 처음 접하면, 여러 토큰을 동시에 추론하는 것은 조합의 경우의 수가 기하급수적으로 늘어나기 때문에 수학적으로 비효율적이고 구현이 불가능하다고 생각할 수 있습니다.</p>
<p>실제로 MTP는 여러 토큰을 한 번에 생성하는 것이 아니라, 작고 빠른 초안(draft) 모델이 예측을 생성하면 크고 정확한 검증(verify) 모델이 그 결과를 확인하는 방식으로 동작합니다. 이 과정은 <strong>추측성 디코딩(Speculative Decoding)</strong>이라고도 불립니다.</p>
<ul>
<li><strong>초안 모델</strong>이 &ldquo;오늘 날씨가&quot;라는 문장 뒤에 (&ldquo;아주&rdquo;, &ldquo;좋을&rdquo;, &ldquo;것&rdquo;, &ldquo;같아요&rdquo;)라는 예측을 빠르게 생성합니다.</li>
<li><strong>검증 모델</strong>이 (&ldquo;아주&rdquo;, &ldquo;좋을&rdquo;, &ldquo;것&rdquo;, &ldquo;같아요&rdquo;)가 모두 맞다고 확인하면, 후보 시퀀스 전체를 채택합니다.</li>
<li><strong>검종 모델</strong>이 (&ldquo;아주&rdquo;, &ldquo;맑을&rdquo;, &hellip;)을 예측한다면,  &ldquo;맑을&rdquo; 까지만 채택하고 다시 초안 모델을 사용하여 예측을 생성합니다.</li>
</ul>
<p>이 방법이 효율적인 이유는 생성(Generation)과 검증(Verification)의 속도 차이에 있습니다. 일반적인 모델은 다음 토큰을 예측하기 위해 순차적으로 연산을 반복해야 하지만, 이미 만들어진 문장이 정답인지 &lsquo;검증&rsquo;하는 작업은 단 한 번의 연산으로 병렬 처리할 수 있기 때문입니다. 결과적으로 여러 토큰을 순차적으로 생성하는 것보다 훨씬 빠른 속도를 달성할 수 있습니다.</p>
<hr>
<h2 id="결론">결론</h2>
<p>Songlim Yang 연구자의 기여(contribution)를 바탕으로 Qwen팀이 유의미한 성과를 만들어내면서, 대형 언어 모델(LLM) 분야에 새로운 패러다임이 열리고 있는 것 같습니다. 덕분에 모델 학습에 필요한 막대한 비용이 줄어들었고, 추론 속도와 메모리 사용량 또한 획기적으로 개선되었습니다.</p>
<p>개인적으로 이 새로운 구조가 기존 트랜스포머보다 훨씬 합리적(reasonable)이라고 느끼는 이유는, 기존 Residual Network과 동일한 철학을 공유하기 때문입니다.</p>
<p>기존의 Scaled Dot-Product Attention은 성능 자체는 뛰어났지만, 어텐션 스코어를 시각화해보면 우리가 직관적으로 예상하는 모습과는 거리가 멀 때가 많았습니다. 또한, 자기회귀(Autoregressive) 생성을 위해 미래 정보를 가리는 마스킹(Masking) 기법 역시 다소 인위적인 제약처럼 느껴졌습니다.</p>
<p>하지만 선형 트랜스포머는 이러한 문제들로부터 비교적 자유롭습니다. <span class="highlight-text primary">상태(state)를 순차적으로 업데이트하는 방식은 훨씬 직관적이고 자연스러운 모델 구조로 보입니다.</span> 이러한 이유로, 저는 앞으로 대부분의 LLM이 선형 트랜스포머 구조를 기반으로 발전할 것이라고 조심스럽게 예측해 봅니다.</p>
<hr>
<h4 id="reference">Reference</h4>
<p>Yang, Songlim (2024) | <a href="https://arxiv.org/pdf/2406.06484">Parallelizing Linear Transformers with the Delta Rule over Sequence Length</a></p>
<p>Yang, Songlim (2024) | <a href="https://arxiv.org/abs/2412.06464">GATED DELTA NETWORKS: IMPROVING MAMBA2 WITH DELTA RULE</a></p>
<p>Qwen Team (2025) | <a href="https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list">Qwen3-Next: Towards Ultimate Training &amp; Inference Efficiency</a></p>
<hr>

              


            </div>
          </div>
          
            <div id="post-footer" class="post-footer main-content-wrap">
              
  <script src="https://utteranc.es/client.js"
    repo="errolseo/blog-comments"
    issue-term="title"
    theme="github-light"
    crossorigin="anonymous"
    async>
  </script>


              <div class="post-actions-wrap">
  <nav >
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://errolseo.github.io/post/llm/1/" data-tooltip="Scaling Laws" aria-label="PREV: Scaling Laws">
          
              <i class="fa fa-angle-left"></i>
              <span class="hide-xs hide-sm text-small icon-ml">PREV</span>
            </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--disabled">
          
              <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
              <i class="fa fa-angle-right"></i>
            </a>
        </li>
      
    </ul>
  </nav>
  <ul class="post-actions post-action-share" >
    
      <li class="post-action hide-lg hide-md hide-sm">
        <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </a>
      </li>
      
    
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#main" aria-label="Back to top">
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
      </a>
    </li>
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#post-footer" aria-label="Leave a comment">
        <i class="far fa-comment"></i>
      </a>
    </li>
  </ul>
</div>

            </div>
          
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; Errol Seo. 2025 All rights reserved.
  </span>
</footer>

      </div>
      
        <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
          <div class="post-actions-wrap">
  <nav >
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://errolseo.github.io/post/llm/1/" data-tooltip="Scaling Laws" aria-label="PREV: Scaling Laws">
          
              <i class="fa fa-angle-left"></i>
              <span class="hide-xs hide-sm text-small icon-ml">PREV</span>
            </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--disabled">
          
              <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
              <i class="fa fa-angle-right"></i>
            </a>
        </li>
      
    </ul>
  </nav>
  <ul class="post-actions post-action-share" >
    
      <li class="post-action hide-lg hide-md hide-sm">
        <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="Share this post">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </a>
      </li>
      
    
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#main" aria-label="Back to top">
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
      </a>
    </li>
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#post-footer" aria-label="Leave a comment">
        <i class="far fa-comment"></i>
      </a>
    </li>
  </ul>
</div>

        </div>
      
    </div>
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>


<script src="https://errolseo.github.io/js/script-vkdwdjvf1vnfmgoiv45dlezn4iclio33g7aveb6wkevdvoymhrzgogftnrv4.min.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" integrity="sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js" integrity="sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>



  </body>
</html>
